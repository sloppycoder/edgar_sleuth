import json
import logging
import logging.config
import multiprocessing
import sys
from logging.handlers import QueueListener
from pathlib import Path
from typing import Iterator

import click
import yaml

from .datastore import execute_query
from .edgar import SECFiling, parse_idx_filename
from .llm.embedding import GEMINI_EMBEDDING_MODEL
from .processor import init_worker, process_filing, process_filing_wrapper
from .trustee import create_search_phrase_embeddings

MAX_ERRORS = 5

logger = logging.getLogger(__name__)

logger_config_path = Path.cwd() / "logger_config.yaml"
if logger_config_path.exists():
    with open(logger_config_path, "r") as f:
        logging.config.dictConfig(yaml.safe_load(f))


def create_filing(
    cik: str = "", accession_number: str = "", idx_filename: str = ""
) -> SECFiling:
    if idx_filename:
        return SECFiling(idx_filename=idx_filename)
    else:
        return SECFiling(cik=cik, accession_number=accession_number)


def enumerate_filings(batch: str, batch_limit: int) -> Iterator[tuple[str, str]]:
    if batch.startswith("@"):
        with open(batch[1:], "r") as f:
            lines = f.readlines()
            lines = [
                line.strip()
                for line in lines
                if line.strip() and not line.startswith("#")
            ]
    else:
        lines = [batch]

    n_processed = 0
    for line in lines:
        if batch_limit and n_processed >= batch_limit:
            print(f"Reached batch limit of {batch_limit}")
            break

        try:
            entry = json.loads(line)
            if entry.get("idx_filename"):
                n_processed += 1
                yield parse_idx_filename(entry["idx_filename"])
            elif entry.get("cik") and entry.get("accession_number"):
                n_processed += 1
                yield entry["cik"], entry["accession_number"]
            else:
                print(f"ERROR: Ignored invalid entry: {entry}")
        except json.JSONDecodeError:
            print(f"ERROR: invalid json {line}")


@click.command()
@click.argument(
    "action",
    type=click.Choice(
        ["initdb", "init", "chunk", "embedding", "extract"], case_sensitive=False
    ),
)
@click.option(
    "--full",
    is_flag=True,
    help="""
Run the predesccor steps prior to the action, e.g.
embedding depends on data generated by chunk, if full is set,
chunk will be run before embedding""",
)
@click.option(
    "--batch",
    help="""JSON string filings to process:
@<jsonl_file> with list of filings or inline string like\n
'{"cik": "1035018", "accession_number": "0001193125-20-000327"}"
'{"idx_filename": "edgar/data/1002427/0001133228-24-004879.txt"}"
""",
)
@click.option(
    "--batch-limit",
    type=int,
    default=0,
    help="Number of records to process in batch mode, 0 means unlimited",
)
@click.option(
    "--tags",
    "tags_str",
    required=False,
    help="tag for text chunks and embedding, required when using chunk, embedding and extract",  # noqa: E501
)
@click.option(
    "--search-tag",
    required=False,
    help="tag for search phrases, required when using search phrase, i.e. init, extract",
)
@click.option(
    "--dimension",
    type=int,
    default=768,
    help="Dimensionality of embeddings. Only applicable when using Gemini API",
)
@click.option(
    "--workers",
    type=int,
    default=1,
    help="Number of works to use for processing. Each worker will process 1 filing at a time",  # noqa: E501
)
@click.option("--dryrun", is_flag=True, help="Print filing only, does not run any action")
# ruff: noqa: C901
def main(
    action: str,
    dryrun: bool,
    full: bool,
    batch: str,
    batch_limit: int,
    tags_str: str,
    search_tag: str,
    dimension: int,
    workers: int,
) -> None:
    # validate tag values first.
    if not search_tag and action in ["init", "initdb", "extract"]:
        raise click.UsageError(
            "--search-tag is required when search phrase are used, e.g. init, extract"
        )
    else:
        search_tag = search_tag.split(",")[0] if search_tag else ""

    if action in ["embedding", "extract"] and (not tags_str or "," in tags_str):
        raise click.UsageError(
            "--tags must be a single tag, without comma, when action is embedding or extract"  # noqa: E501
        )

    if action in ["chunk"] and not tags_str:
        raise click.UsageError("--tags is required when action is chunk")

    tags = tags_str.split(",") if tags_str else []

    print(f"Running {action} with tags {tags} and search tag {search_tag}")

    # hard coded values for now
    text_table_name = "filing_text_chunks"
    embedding_table_name = "filing_chunks_embeddings"
    search_phrase_table_name = "search_phrase_embeddings"
    form_type = "485BPOS"

    answer = "no"
    if action == "initdb":
        answer = input("This will drop all tables, are you sure? (yes/no): ")
        if answer.lower() == "yes":
            for table_name in [
                text_table_name,
                embedding_table_name,
                search_phrase_table_name,
            ]:
                print(f"Dropping table {table_name}")
                execute_query(f"DROP TABLE IF EXISTS {table_name}")

    if action == "init" or (action == "initdb" and answer.lower() == "yes"):
        print("Initializing search phrase embeddings...")
        create_search_phrase_embeddings(
            search_phrase_table_name,
            model=GEMINI_EMBEDDING_MODEL,
            tag=search_tag,
            dimension=dimension,
        )
        return

    actions = [action]
    if full:
        if action == "embedding":
            actions = ["chunk", "embedding"]
        elif action == "extract":
            actions = ["chunk", "embedding", "extract"]

    if dryrun or workers == 1:
        for cik, accession_number in enumerate_filings(batch, batch_limit):
            if dryrun:
                print("Processing Filing({cik},{accession_number})")
                continue

            process_filing(
                actions=actions,
                search_tag=search_tag,
                dimension=dimension,
                cik=cik,
                accession_number=accession_number,
                tags=tags,
                model="gemini-1.5-flash-002",
                text_table_name=text_table_name,
                embedding_table_name=embedding_table_name,
                search_phrase_table_name=search_phrase_table_name,
                form_type=form_type,
            )
    else:
        all_filings = list(enumerate_filings(batch, batch_limit))
        args = [
            {
                "actions": actions,
                "search_tag": search_tag,
                "dimension": dimension,
                "filing": filing,
                "tags": tags,
                "model": "gemini-1.5-flash-002",
                "text_table_name": text_table_name,
                "embedding_table_name": embedding_table_name,
                "search_phrase_table_name": search_phrase_table_name,
                "form_type": form_type,
            }
            for filing in all_filings
        ]

        # create a queue to receive log messages from worker processes
        # https://stackoverflow.com/questions/641420/how-should-i-log-while-using-multiprocessing-in-python
        logging_q = multiprocessing.Queue()
        handler = logging.getHandlerByName("console")  # defined in logger_config.yaml
        q_listener = QueueListener(logging_q, handler)  # pyright: ignore
        q_listener.start()

        with multiprocessing.Pool(
            workers,
            init_worker,
            (
                logging_q,
                logging.DEBUG,
            ),
        ) as pool:
            pool.map(process_filing_wrapper, args)

        q_listener.stop()


if __name__ == "__main__":
    main(sys.argv[1:])
